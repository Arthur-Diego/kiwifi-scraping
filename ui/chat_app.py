import streamlit as st
import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from service.rag_service import run_rag
from repository.qdrant_repository import QdrantRetriever

st.set_page_config(page_title="Analise de m√©tricas", page_icon="ü§ñ", layout="wide")

# ======================
# CONFIGURA√á√ïES INICIAIS
# ======================
st.title("ü§ñ RAG Context Chat")
st.markdown("Converse com o **seu contexto vetorial (Qdrant)** usando o poder do GPT!")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "model" not in st.session_state:
    st.session_state.model = "gpt-4o-mini"

# ======================
# SIDEBAR DE CONFIGURA√á√ÉO
# ======================
st.sidebar.header("‚öôÔ∏è Configura√ß√µes")

st.sidebar.markdown("### üîç Busca Vetorial")
top_k = st.sidebar.slider("Quantidade de contextos (top_k):", 3, 20, 8)

st.sidebar.markdown("### üß† Modelo LLM")
model = st.sidebar.selectbox(
    "Selecione o modelo:",
    ["gpt-5", "gpt-4o-mini", "gpt-4-turbo"],
    index=["gpt-4o-mini", "gpt-5", "gpt-4-turbo"].index(st.session_state.model) if st.session_state.model else 0
)
st.session_state.model = model

temperature = st.sidebar.slider("Temperatura:", 0.0, 1.0, 0.7, 0.1)

if st.sidebar.button("üßπ Limpar hist√≥rico"):
    st.session_state.chat_history = []
    st.rerun()

# ======================
# INTERFACE PRINCIPAL
# ======================

for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

prompt = st.chat_input("Digite sua pergunta...")

if prompt:
    # Exibe a mensagem do usu√°rio
    st.chat_message("user").markdown(prompt)
    st.session_state.chat_history.append({"role": "user", "content": prompt})

    # Busca contextos + gera resposta
    with st.chat_message("assistant"):
        with st.spinner("üîé Buscando contextos e gerando resposta..."):
            try:
                retriever = QdrantRetriever()
                contexts = retriever.search(prompt, top_k=top_k)
                answer = run_rag(prompt)

                st.markdown(answer)
                with st.expander("üìö Contextos utilizados"):
                    for i, ctx in enumerate(contexts, 1):
                        st.markdown(f"**Trecho {i}:**")
                        st.markdown(f"> {ctx}")

                st.session_state.chat_history.append({"role": "assistant", "content": answer})

            except Exception as e:
                st.error(f"‚ùå Erro ao processar consulta: {e}")

# ======================
# RODAP√â
# ======================
st.markdown("---")
st.caption("Desenvolvido com ‚ù§Ô∏è usando Streamlit + Qdrant + LangChain + OpenAI")
